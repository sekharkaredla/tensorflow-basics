{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import split_dataset_with_ratio\n",
    "\n",
    "class BatchDataset:\n",
    "    def __init__(self,ratio,batch_size):\n",
    "        data = split_dataset_with_ratio(ratio)\n",
    "        self.data_train = data[0]\n",
    "        self.labels_train = data[1]\n",
    "        self.data_test = data[2]\n",
    "        self.labels_test = data[3]\n",
    "        self.counter = 0\n",
    "        self.data_present = True\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.counter+self.batch_size<len(self.data_train):\n",
    "            return (self.data_train[self.counter:self.counter+self.batch_size],self.labels_train[self.counter:self.counter+self.batch_size])\n",
    "            self.counter += self.batch_size\n",
    "        else:\n",
    "            self.data_present = False\n",
    "            return (self.data_train[self.counter:],self.labels_train[self.counter:])\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return (self.data_test,self.labels_test)\n",
    "\n",
    "    def get_details(self):\n",
    "        print len(self.data_train), len(self.labels_train), len(self.data_test), len(self.labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bais:\n",
    "    def __init__(self,size):\n",
    "        self.biases = tf.Variable(tf.constant(0.04, shape=[size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, input, num_of_input_channels, conv_filter_size, num_of_filters):\n",
    "        self.weights = Weight(shape= [conv_filter_size, conv_filter_size, num_of_input_channels, num_of_filters])\n",
    "        self.baises = Bais(size=num_of_filters)\n",
    "\n",
    "\n",
    "        self.cnn = tf.nn.conv2d(input= input, filter= self.weights.weights, strides= [1, 1, 1, 1], padding= 'SAME')\n",
    "\n",
    "        self.cnn += self.baises.biases\n",
    "\n",
    "        self.cnn = tf.nn.max_pool(value= self.cnn, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding= 'SAME')\n",
    "\n",
    "        self.cnn = tf.nn.relu(self.cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_layer(input, no_of_inputs, no_of_outputs):\n",
    "    weights = Weight(shape=[no_of_inputs, no_of_outputs])\n",
    "    biases = Bais(no_of_outputs)\n",
    "    layer = tf.matmul(input, weights.weights) + biases.biases\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    # if it is 'label1' it will be represented as [1, 0, 0] in one_hot_encoder. At a time only one output is hot\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[range(n_labels),labels] = 1\n",
    "    one_hot_encode.astype(float)\n",
    "    return one_hot_encode\n",
    "\n",
    "def split_dataset_with_ratio(ratio):\n",
    "    data = []\n",
    "    labels = []\n",
    "    flower_data_folder = \"/home/sekhar/EXTRAS/flowers-recognition/flowers_reshaped/\"\n",
    "    for each_flower_image in glob.glob(flower_data_folder+\"*.jpg\"):\n",
    "        image_data = cv2.imread(each_flower_image)\n",
    "        image_data = image_data.astype(float)\n",
    "        image_label = each_flower_image.split('/')[-1].split('_')[0]\n",
    "        data.append(image_data)\n",
    "        labels.append(image_label)\n",
    "        print image_label\n",
    "        print image_data\n",
    "\n",
    "    data, labels = shuffle(data, labels, random_state=1)\n",
    "    labelEncoder = LabelEncoder()\n",
    "    labelEncoder.fit(labels)\n",
    "    labels = labelEncoder.transform(labels)\n",
    "    labels = one_hot_encode(labels=labels)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=ratio, random_state=42)\n",
    "    return data_train, labels_train, data_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weight:\n",
    "    def __init__(self,shape):\n",
    "        self.weights = tf.Variable(tf.truncated_normal(shape = shape, stddev=0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc6895bad1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMAGE_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFILTERS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0moutput_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNO_OF_FLOWERS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "FILTERS = 3\n",
    "NO_OF_FLOWERS = 5\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT,IMAGE_WIDTH,FILTERS])\n",
    "output_true = tf.placeholder(tf.float32, shape=[None, NO_OF_FLOWERS])\n",
    "\n",
    "\n",
    "\n",
    "layer1 = CNN(input=input, num_of_input_channels=3, conv_filter_size=8, num_of_filters=3).cnn\n",
    "layer2 = CNN(input=layer1, num_of_input_channels=3, conv_filter_size=8, num_of_filters=3).cnn\n",
    "layer3 = CNN(input=layer2, num_of_input_channels=3, conv_filter_size= 8, num_of_filters= 3).cnn\n",
    "layer4 = CNN(input=layer3, num_of_input_channels=3, conv_filter_size= 8, num_of_filters= 3).cnn\n",
    "\n",
    "layer_flat = flatten(layer4)\n",
    "\n",
    "layer_fully_connected = create_fully_connected_layer(input=layer_flat,no_of_inputs=layer_flat.get_shape()[1:4].num_elements(),no_of_outputs=5)\n",
    "\n",
    "# fileWriter = tf.summary.FileWriter(\".\",sess.graph)\n",
    "print(layer_fully_connected)\n",
    "\n",
    "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= layer_fully_connected, labels= output_true))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost_function)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batchDataset = BatchDataset(0.30,4)\n",
    "print \"dataset content details:\"\n",
    "batchDataset.get_details()\n",
    "\n",
    "# for each_epoch in range(EPOCHS):\n",
    "#     sess.run(optimizer, feed_dict={input: data_train, output_true: labels_train})\n",
    "#     cost = sess.run(cost_function,feed_dict={input:data_train, output_true:labels_train})\n",
    "#     correct_predictions = tf.equal(tf.argmax(layer_fully_connected,1),tf.argmax(output_true,1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "#     pred_outputs = sess.run(layer_fully_connected,{input:data_test})\n",
    "#     mean_square_error = tf.reduce_mean(tf.square(pred_outputs - labels_test))\n",
    "#     mean_square_error_value = sess.run(mean_square_error)\n",
    "#     accuracy_value = sess.run(accuracy, {input: data_test, output_true: labels_test})\n",
    "#     print \"epoch number : \"+ str(each_epoch) + \" - cost : \"+ str(cost)+ \" - mse : \"+ str(mean_square_error_value)+\" - accuracy : \"+str(accuracy_value)\n",
    "\n",
    "\n",
    "for each_epoch in range(EPOCHS):\n",
    "    batchDataset.data_present = True\n",
    "    batchDataset.counter = 0\n",
    "    data_test, labels_test = batchDataset.get_test_data()\n",
    "    while(batchDataset.data_present):\n",
    "        data_train, labels_train = batchDataset.get_next()\n",
    "        print (data_train,labels_train)\n",
    "        sess.run(optimizer, feed_dict={input: data_train, output_true: labels_train})\n",
    "        cost = sess.run(cost_function,feed_dict={input:data_train, output_true:labels_train})\n",
    "        correct_predictions = tf.equal(tf.argmax(layer_fully_connected,1),tf.argmax(output_true,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "        pred_outputs = sess.run(layer_fully_connected,{input:data_test})\n",
    "        mean_square_error = tf.reduce_mean(tf.square(pred_outputs - labels_test))\n",
    "        mean_square_error_value = sess.run(mean_square_error)\n",
    "        accuracy_value = sess.run(accuracy, {input: data_test, output_true: labels_test})\n",
    "        print \"epoch number : \"+ str(each_epoch) + \" - cost : \"+ str(cost)+\" - mse : \"+str(mean_square_error_value)+\" - accuracy : \"+str(accuracy_value)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"/home/sekhar/EXTRAS/tensorflow-basics/Flower-Dataset\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
